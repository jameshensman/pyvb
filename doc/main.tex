\documentclass{article}
\title{pyvb: Variational Bayesian Networks in python}
\author{James Hensman, Mike Dewar}
\date{\today}


\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{natbib}

\newcommand{\const}{\textit{const.}}
\newcommand{\tr}{\textit{tr}}
\newcommand{\qs}{q^\star}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\begin{document}
\maketitle

\section{Bayesian Networks}
\section{Variational Bayes}
Some directed probabilistic models cannot be solved in closed form. A common approach is to use a Monte Carlo smapling method, (see for example the excellent {\em pymc} project \cite{pymc}).  In the limit of an infinite number of samples, the Monte Carlo method provides perfect inference of the posterior probability density (see any statistics textbook for a proof), and a high number of samples provides an {\em approximation} to the posterior.  

Similarly in Variation Bayes, we seek an approximation to the posterior given by some {\em factorial} distribution $q$:
\begin{equation}
p(A,B,\ldots) \approx q_A(A) q_B(B) \ldots
\end{equation}
for convenience, $q_A(A)$ is often written as $q(A)$, etcetera. 

The distribution $q$ is 'optimised' by finding the distribution $q$ which minimises the KL divergence between $p(A,B,\ldots)$ and $q(A,B,\ldots)$.  After some long and arduous mathematics, \citet{Bishop2006prm} yeilds

\begin{equation}
\ln \qs(A) = \< \ln p(A,B,\ldots)\>_{q(\bar A)}
\label{eq:basic_VB_update}
\end{equation}
\section{Network Structure in pyvb}
In pyvb, all random variables are represented by an instance of an appropriate class. There are classes for Gaussian nodes, Gamma nodes and Wishart nodes. More random variables are to be implmented ion future (see TODO).

Additionally, {\em operations} are represented by instances of classes. There are classes to represent addtion, multiplication and horizontal stacking (hstack).  This gives us an easy way to deal with co-parents.  Some of the code in these classes is quite compicated, since this is where messages are 'modified' according to the operation. 

The basic node class, form which Gaussian nodes inherrit, is capable of operations such as addition amd multiplication. This means that one can instantiate a pair of nodes, and then add them using the '+' operator: this creates and returns an instance of the Addition class, with the two original nodes as parents. 

Each node then has two 'parents': in a Gaussian node the 'mean\_parent' and 'precision\_parent' attributes are pointers to the apppropriate nodes; in an Addition node the attributes 'x1' and 'x2' are pointers to the two nodes to be added; in the Gamma node the attributes 'a0' and 'b0' are pointers to the appropriate nodes and so on.

We have bastardised the standard nomenclature slightly: strictly a node can have more than two 'true' parents. In our implementation, operation nodes are used to facilitate this. For example, the graphical model if Figure \ref{fig:gm_true_bastard}(a) represents a Gaussian node $C$ and it's three parents $A$,$B$ and $\Lambda_C$.  The diagram in Figure \ref{fig:gm_true_bastard}(b) shows how an operation node (in this case addition) can be used to represent this. 

\begin{figure}
\centering
\begin{minipage}{0.4\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{images/gm_true}
(a)
\end{center}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{images/gm_bastard}
(b)
\end{center}
\end{minipage}
\caption{(a) A simple Graphical model.  (b) A diagram showing the introduction of an operation node.}
\label{fig:gm_true_bastard}
\end{figure}



Each node can have an arbitrary number of children: again, children can be eitherGaussian nodes or operation nodes. 

\fbox{\parbox{0.8\textwidth}{ The node instances are not 'aware' of the nature of the nodes around them in the network. They simply expect to be passed appropriate messages when updating. }}






\section{A Gaussian Node}
Consider the simple DAG in Figure \ref{fig:simple_DAG}.  $A$ is the 'mean\_parent' of $B$, and $\Lambda_B$ is the 'precision\_parent' of $B$. That, is, wwe can write the joint probability of the nodes as
\begin{equation}
\begin{split}
p(A,B,\Lambda_B) &= p(A) p(B | A, \Lambda_B) p(\Lambda_B)\\
&= \mathcal N (A|\mu_A, \Lambda_A) \mathcal N (B | A, \Lambda_B) \mathcal {Ga}(\Lambda_B | \nu_0, W_0)
\end{split}
\end{equation}
assuming that the priors on $A$ and $\Lambda_B$ have fixed parameters: hopefully the notation is obvious.  

\begin{figure}
\centering
\includegraphics{images/simple_DAG}
\caption{A simple network with two Gausian nodes.}
\label{fig:simple_DAG}
\end{figure}

It is trivial to write out the log of the joint probability: for convenience, we write it as $-2 \ln p(.)$:
\begin{equation}
-2 \ln p(A,B,\Lambda_B) = (A-\mu_A)^\top\lambda_A(A-\mu_A) + (B-A)^\top\Lambda_B(B-A) + \tr (W_0^{-1}\Lambda_B) + \ln|\Lambda_B| + \const
\end{equation}
where we have followed \cite{Bishop2006prm} in incorporating irrelevant terms into {\em const}.  

We are now ready to apply the variational updates to all othe nodes. Recalling (\ref{eq:basic_VB_update}):
\begin{equation}
\begin{split}
-2 \ln \qs (A) &= \big< A^\top\lambda_A A -2A^\top\Lambda_A\mu_A + A^\top\Lambda_B A  - 2A^\top \Lambda_B B \big>\\
 &= A^\top(\Lambda_A + \<\Lambda_B\>)A -2A^\top(\Lambda_A\mu_A + \<\Lambda_B\> \<B\>)
\end{split}
\end{equation}
we recognise this as the log of a Gaussian with precision $\Lambda_A + \<\Lambda_B\>$ and mean $(\Lambda_A + \<\Lambda_B\>)^{-1}(\Lambda_A\mu_A + \<\Lambda_B\> \<B\>)$.  



\section{Operations}
\subsection{Addition}
\subsection{Multiplication}
Consider the updating of a node $A$, with parent nodes $\mu_A$ and $\Lambda_A$.  A has a child node $C$, with co-parents $\Lambda_C$ and (through multipication) $B$.  The situation is shown in figure \ref{fig:mult_markov}
note that the nodes $B$,$C$,$\mu_A$ and $\Lambda_A$ form the Markov blanket (is this the correct term? I think so) of A, and so no other information is required for the update - even if the network stretched for many nodes in any direction.  

We're assuming the $A$ is right-multiplied by $B$, i.e. %we have $p(C) = \mathcal{N}(C | AB, \Lambda_C)$.  The log joint probability is:

\begin{equation}
-2 \log p(.) = 
\end{equation}


\begin{figure}
\includegraphics{images/mult_markov}
\caption{The markov blanket of the node A}
\label{fig:mult_markov}
\end{figure}

\subsection{Hstacking}

\section{Precision Nodes}
\section{Gamma}
\section{Diagonal Gamma}
\section{Wishart}

\section{Examples}
\subsection{Linear Regression}
\subsection{Principal Component Analysis}

\subsection{Linear Dynamic System}
In the KF, we do
\begin{equation}
p(x_t) = \int p(x_t | x_{t-1}) p(x_t | y_t) p(x_{t-1}) d x_{t-1}
\end{equation}

In the variational equivalent, we do
\begin{equation}
\begin{split}
\ln q^\star(x_t) &= \int \ln \left( p(x_t | x_{t-1}) p(x_t | y_t) \right) q(x_{t-1}) d x_{t-1}\\
            &= \left<  \ln  p(x_t | x_{t-1})  + \ln p(x_t | y_t)  \right>_{q(x_{t-1})}
\end{split}
\end{equation}


\section{Appendix}
\subsection{The Gaussian Distribution}
\subsection{The Gamma Distribution}
\subsection{The Wishart Distribution}

\end{document}